{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z4tbKmWW_-xO"
      },
      "outputs": [],
      "source": [
        "# Importar librerías necesarias\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import datetime\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC-UNQg9EXIP",
        "outputId": "8551de2d-80b2-4dc1-a903-2719e859c00d"
      },
      "outputs": [],
      "source": [
        "video_dir = 'C:\\\\Tesis\\\\TestErgo\\\\muestra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sgfbq09AE0Gt"
      },
      "outputs": [],
      "source": [
        "def load_videos(video_dir):\n",
        "    videos = []\n",
        "    for filename in os.listdir(video_dir):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            videos.append(os.path.join(video_dir, filename))\n",
        "    return videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar videos\n",
        "videos = load_videos(video_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsTSAyoiEtPf",
        "outputId": "cd8f6c05-5307-440a-bad9-eba9014aa2c9"
      },
      "outputs": [],
      "source": [
        "# Función para extraer frames de los videos\n",
        "def extract_frames(video_path, interval=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % interval == 0:\n",
        "            frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "etiquetado.mp4\n"
          ]
        }
      ],
      "source": [
        "# Obtener y mostrar los nombres de todos los videos\n",
        "for video_path in videos:\n",
        "    video_name = os.path.basename(video_path)\n",
        "    print(video_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4W3aozRXbO8v"
      },
      "outputs": [],
      "source": [
        "# Inicializar MediaPipe para detección de puntos clave\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "# Función para calcular el ángulo entre tres puntos\n",
        "def calculate_angle(a, b, c, d=None):\n",
        "    if d is None:\n",
        "        # Calcula el ángulo entre a, b y c\n",
        "        a = np.array(a)  # Primer punto\n",
        "        b = np.array(b)  # Segundo punto (vértice)\n",
        "        c = np.array(c)  # Tercer punto\n",
        "\n",
        "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
        "        angle = np.abs(radians * 180.0 / np.pi)\n",
        "\n",
        "        if angle > 180.0:\n",
        "            angle = 360.0 - angle\n",
        "\n",
        "        return angle\n",
        "    else:\n",
        "        # Calcula el ángulo entre a, b, c y d\n",
        "        a = np.array(a)  # Primer punto\n",
        "        b = np.array(b)  # Segundo punto (vértice)\n",
        "        c = np.array(c)  # Tercer punto\n",
        "        d = np.array(d)  # Cuarto punto\n",
        "\n",
        "        radians = np.arctan2(d[1] - c[1], d[0] - c[0]) - np.arctan2(b[1] - a[1], b[0] - a[0])\n",
        "        angle = np.abs(radians * 180.0 / np.pi)\n",
        "\n",
        "        if angle > 180.0:\n",
        "            angle = 360.0 - angle\n",
        "\n",
        "        return angle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para detectar puntos clave usando MediaPipe\n",
        "def detect_keypoints(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
        "    keypoints_list = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(image_rgb)\n",
        "        if results.pose_landmarks:\n",
        "            keypoints = [(lm.x, lm.y, lm.z) for lm in results.pose_landmarks.landmark]\n",
        "            keypoints_list.append(keypoints)\n",
        "        else:\n",
        "            keypoints_list.append([])  # Agregar una lista vacía si no se detectan keypoints\n",
        "\n",
        "    cap.release()\n",
        "    pose.close()\n",
        "    return keypoints_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'def analyze_videos_in_folder(video_folder, output_json_path):\\n    all_analysis_results = []\\n    \\n    for video_file in os.listdir(video_folder):\\n        if video_file.endswith((\\'.mp4\\', \\'.avi\\', \\'.mov\\')):\\n            video_path = os.path.join(video_folder, video_file)\\n            video_name = os.path.splitext(video_file)[0]\\n            \\n            keypoints_list = detect_keypoints(video_path)\\n            cap = cv2.VideoCapture(video_path)\\n            fps = int(cap.get(cv2.CAP_PROP_FPS))\\n            cap.release()\\n\\n            analysis_results = analyze_keypoints(keypoints_list, video_name, fps)\\n            all_analysis_results.extend(analysis_results)\\n            draw_keypoints_and_angles(video_path, analysis_results, output_folder, output_video_folder, json_folder)\\n    \\n    combined_json_path = os.path.join(json_folder, \\'Resultado_analisis_combinados.json\\')\\n    with open(output_json_path, \\'w\\') as json_file:\\n        json.dump(all_analysis_results, json_file, indent=4)\\n\\n    print(f\"Resultados del análisis exportados a: {output_json_path}\")\\n\\n# Ejemplo de uso:\\n#video_folder = \\'C:\\\\Tesis\\\\TestErgo\\\\muestra\\'\\nvideo_folder = video_dir #\\'C:\\\\Tesis\\\\TestErgo\\\\VIDEOS MAQUILAS\\'\\noutput_json_path = \\'C:\\\\Tesis\\\\TestErgo\\\\analisis_videos.json\\'\\n\\nanalyze_videos_in_folder(video_folder, output_json_path)'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Función para analizar videos masivos y guardar resultados en un solo JSON\n",
        "'''def analyze_videos_in_folder(video_folder, output_json_path):\n",
        "    all_analysis_results = []\n",
        "    \n",
        "    for video_file in os.listdir(video_folder):\n",
        "        if video_file.endswith(('.mp4', '.avi', '.mov')):\n",
        "            video_path = os.path.join(video_folder, video_file)\n",
        "            video_name = os.path.splitext(video_file)[0]\n",
        "            \n",
        "            keypoints_list = detect_keypoints(video_path)\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "            cap.release()\n",
        "\n",
        "            analysis_results = analyze_keypoints(keypoints_list, video_name, fps)\n",
        "            all_analysis_results.extend(analysis_results)\n",
        "            draw_keypoints_and_angles(video_path, analysis_results, output_folder, output_video_folder, json_folder)\n",
        "    \n",
        "    combined_json_path = os.path.join(json_folder, 'Resultado_analisis_combinados.json')\n",
        "    with open(output_json_path, 'w') as json_file:\n",
        "        json.dump(all_analysis_results, json_file, indent=4)\n",
        "\n",
        "    print(f\"Resultados del análisis exportados a: {output_json_path}\")\n",
        "\n",
        "# Ejemplo de uso:\n",
        "#video_folder = 'C:\\\\Tesis\\\\TestErgo\\\\muestra'\n",
        "video_folder = video_dir #'C:\\\\Tesis\\\\TestErgo\\\\VIDEOS MAQUILAS'\n",
        "output_json_path = 'C:\\\\Tesis\\\\TestErgo\\\\analisis_videos.json'\n",
        "\n",
        "analyze_videos_in_folder(video_folder, output_json_path)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_keypoints(video_path):\n",
        "    keypoints_list = detect_keypoints(video_path)\n",
        "    fps = None\n",
        "    analysis_results = []\n",
        "\n",
        "    for frame_idx, keypoints in enumerate(keypoints_list):\n",
        "        second = frame_idx // fps if fps else 0\n",
        "\n",
        "        if not keypoints:\n",
        "            analysis_results.append({\n",
        "                'segundo': second,\n",
        "                'frame': frame_idx,\n",
        "                'video_name': None,\n",
        "                'angulo_hombro_izquierdo': None,\n",
        "                'angulo_del_hombro_derecho': None,\n",
        "                'angulo_codo_izquierdo': None,\n",
        "                'angulo_codo_derecho': None,\n",
        "                'angulo_de_muneca_izquierda': None,\n",
        "                'angulo_de_muneca_derecha': None,\n",
        "                'angulo_mano_izquierdo': None,\n",
        "                'angulo_mano_derecho': None,\n",
        "                'posicion_hombro_izquierdo': None,\n",
        "                'posicion_hombro_derecho': None,\n",
        "                'posicion_codo_izquierdo': None,\n",
        "                'posicion_codo_derecho': None,\n",
        "                'posicion_muneca_izquierda': None,\n",
        "                'posicion_muneca_derecha': None,\n",
        "                'posicion_mano_izquierda': None,\n",
        "                'posicion_mano_derecha': None,\n",
        "                'keypoints': []\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Extraer puntos clave izquierdos\n",
        "        left_shoulder = keypoints[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "        left_elbow = keypoints[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
        "        left_wrist = keypoints[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
        "        left_hand = keypoints[mp_pose.PoseLandmark.LEFT_INDEX.value]\n",
        "\n",
        "        # Extraer puntos clave derechos\n",
        "        right_shoulder = keypoints[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "        right_elbow = keypoints[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
        "        right_wrist = keypoints[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
        "        right_hand = keypoints[mp_pose.PoseLandmark.RIGHT_INDEX.value]\n",
        "\n",
        "        # Calcular ángulos izquierdos\n",
        "        left_shoulder_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
        "        left_elbow_angle = calculate_angle(left_elbow, left_wrist, left_hand)\n",
        "        left_wrist_angle = calculate_angle(left_elbow, left_wrist, left_hand)\n",
        "        left_hand_angle = calculate_angle(left_elbow, left_wrist, left_hand)\n",
        "\n",
        "        # Calcular ángulos derechos\n",
        "        right_shoulder_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
        "        right_elbow_angle = calculate_angle(right_elbow, right_wrist, right_hand)\n",
        "        right_wrist_angle = calculate_angle(right_elbow, right_wrist, right_hand)\n",
        "        right_hand_angle = calculate_angle(right_elbow, right_wrist, right_hand)\n",
        "\n",
        "        # Si no se ha establecido aún el FPS, se establece ahora\n",
        "        if fps is None:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "            cap.release()\n",
        "\n",
        "        analysis_results.append({\n",
        "            'segundo': second,\n",
        "            'frame': frame_idx,\n",
        "            'analisis_video': video_path,\n",
        "            'angulo_hombro_izquierdo': left_shoulder_angle,\n",
        "            'angulo_del_hombro_derecho': right_shoulder_angle,\n",
        "            'angulo_codo_izquierdo': left_elbow_angle,\n",
        "            'angulo_codo_derecho': right_elbow_angle,\n",
        "            'angulo_de_muneca_izquierda': left_wrist_angle,\n",
        "            'angulo_de_muneca_derecha': right_wrist_angle,\n",
        "            'angulo_mano_izquierdo': left_hand_angle,\n",
        "            'angulo_mano_derecho': right_hand_angle,\n",
        "            'posicion_hombro_izquierdo': left_shoulder,\n",
        "            'posicion_hombro_derecho': right_shoulder,\n",
        "            'posicion_codo_izquierdo': left_elbow,\n",
        "            'posicion_codo_derecho': right_elbow,\n",
        "            'posicion_muneca_izquierda': left_wrist,\n",
        "            'posicion_muneca_derecha': right_wrist,\n",
        "            'posicion_mano_izquierda': left_hand,\n",
        "            'posicion_mano_derecha': right_hand,\n",
        "            'keypoints': keypoints\n",
        "        })\n",
        "\n",
        "    return analysis_results, fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_keypoints_and_angles(video_path, output_folder, output_video_folder, json_folder):\n",
        "    analysis_results, fps = analyze_keypoints(video_path)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error al abrir el video: {video_path}\")\n",
        "        return\n",
        "    \n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    \n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    video_frame_folder = os.path.join(output_folder, video_name)\n",
        "    output_video_name = f\"{video_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4\"\n",
        "    output_video_path = os.path.join(output_video_folder, output_video_name)\n",
        "    json_filename = f\"{video_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    json_path = os.path.join(json_folder, json_filename)\n",
        "\n",
        "    # Crear carpetas si no existen\n",
        "    if not os.path.exists(video_frame_folder):\n",
        "        os.makedirs(video_frame_folder)\n",
        "    if not os.path.exists(output_video_folder):\n",
        "        os.makedirs(output_video_folder)\n",
        "    if not os.path.exists(json_folder):\n",
        "        os.makedirs(json_folder)\n",
        "\n",
        "    # Crear el video marcado\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "    for i in range(len(analysis_results)):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        keypoints = analysis_results[i]['keypoints']\n",
        "        left_shoulder_angle = analysis_results[i]['angulo_hombro_izquierdo']\n",
        "        right_shoulder_angle = analysis_results[i]['angulo_del_hombro_derecho']\n",
        "        left_elbow_angle = analysis_results[i]['angulo_codo_izquierdo']\n",
        "        right_elbow_angle = analysis_results[i]['angulo_codo_derecho']\n",
        "        left_wrist_angle = analysis_results[i]['angulo_de_muneca_izquierda']\n",
        "        right_wrist_angle = analysis_results[i]['angulo_de_muneca_derecha']\n",
        "        left_hand_angle = analysis_results[i]['angulo_mano_izquierdo']\n",
        "        right_hand_angle = analysis_results[i]['angulo_mano_derecho']\n",
        "\n",
        "        # Dibujar puntos clave y conexiones con el color original\n",
        "        if keypoints:\n",
        "            for j, point in enumerate(keypoints):\n",
        "                cv2.circle(frame, (int(point[0] * frame.shape[1]), int(point[1] * frame.shape[0])), 5, (0, 255, 0), -1)\n",
        "\n",
        "                # Añadir texto con información relevante junto a cada punto\n",
        "                if j == mp_pose.PoseLandmark.LEFT_SHOULDER.value:\n",
        "                    cv2.putText(frame, f'Angulo del hombro izquierdo: {int(left_shoulder_angle) if left_shoulder_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "                elif j == mp_pose.PoseLandmark.RIGHT_SHOULDER.value:\n",
        "                    cv2.putText(frame, f'Angulo del hombro derecho: {int(right_shoulder_angle) if right_shoulder_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "                elif j == mp_pose.PoseLandmark.LEFT_ELBOW.value:\n",
        "                    cv2.putText(frame, f'Angulo del codo izquierdo: {int(left_elbow_angle) if left_elbow_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "                elif j == mp_pose.PoseLandmark.RIGHT_ELBOW.value:\n",
        "                    cv2.putText(frame, f'Angulo del codo derecho: {int(right_elbow_angle) if right_elbow_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "                elif j == mp_pose.PoseLandmark.LEFT_WRIST.value:\n",
        "                    cv2.putText(frame, f'Angulo de la muneca izquierda: {int(left_wrist_angle) if left_wrist_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "                elif j == mp_pose.PoseLandmark.RIGHT_WRIST.value:\n",
        "                    cv2.putText(frame, f'Angulo de la muneca derecha: {int(right_wrist_angle) if right_wrist_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "                elif j == mp_pose.PoseLandmark.LEFT_INDEX.value:\n",
        "                    cv2.putText(frame, f'Angulo de la mano izquierda: {int(left_hand_angle) if left_hand_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "                elif j == mp_pose.PoseLandmark.RIGHT_INDEX.value:\n",
        "                    cv2.putText(frame, f'Angulo de la mano derecha: {int(right_hand_angle) if right_hand_angle else 0}',\n",
        "                                (int(point[0] * frame.shape[1]) + 10, int(point[1] * frame.shape[0]) - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "        # Guardar el frame en la carpeta de salida con el nombre del video seguido de su numeración\n",
        "        output_frame_path = os.path.join(video_frame_folder, f'{video_name}_frame_{frame_count:04d}.png')\n",
        "        cv2.imwrite(output_frame_path, frame)\n",
        "        frame_count += 1\n",
        "\n",
        "        # Guardar el frame en el video marcado\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Guardar el archivo JSON con los resultados del análisis\n",
        "    with open(json_path, 'w') as json_file:\n",
        "        json.dump(analysis_results, json_file, indent=4)\n",
        "\n",
        "    print(f\"Frames exportados a: {video_frame_folder}\")\n",
        "    print(f\"Video exportado a: {output_video_path}\")\n",
        "    print(f\"JSON exportado a: {json_path}\")\n",
        "    return video_frame_folder, output_video_path, json_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Tesis\\TesisOCRA\\TesisCheckListOCRA\\python\\venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frames exportados a: C:\\Tesis\\TestErgo\\resultados\\etiquetado\n",
            "Video exportado a: C:\\Tesis\\TestErgo\\videoMarcado\\etiquetado_20240718_200217.mp4\n",
            "JSON exportado a: C:\\Tesis\\TestErgo\\archivos_json\\etiquetado_20240718_200217.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('C:\\\\Tesis\\\\TestErgo\\\\resultados\\\\etiquetado',\n",
              " 'C:\\\\Tesis\\\\TestErgo\\\\videoMarcado\\\\etiquetado_20240718_200217.mp4',\n",
              " 'C:\\\\Tesis\\\\TestErgo\\\\archivos_json\\\\etiquetado_20240718_200217.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_path = videos[0]\n",
        "output_folder = 'C:\\\\Tesis\\\\TestErgo\\\\resultados'\n",
        "output_video_folder = 'C:\\\\Tesis\\\\TestErgo\\\\videoMarcado'\n",
        "json_folder = 'C:\\\\Tesis\\\\TestErgo\\\\archivos_json'\n",
        "\n",
        "draw_keypoints_and_angles(video_path, output_folder, output_video_folder, json_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Factor Recuperacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duración del video: 189.08695652173913 segundos\n",
            "Pausas detectadas:\n"
          ]
        }
      ],
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "def detectar_movimiento(frame, pose, umbral_movimiento=0.05):\n",
        "    # Convertir la imagen de BGR a RGB\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(frame_rgb)\n",
        "    \n",
        "    # Si no se detecta ninguna pose, considerar que no hay movimiento\n",
        "    if not results.pose_landmarks:\n",
        "        return False\n",
        "\n",
        "    # Extraer los puntos clave de la pose detectada\n",
        "    keypoints = results.pose_landmarks.landmark\n",
        "    keypoints_array = np.array([[kp.x, kp.y, kp.z] for kp in keypoints])\n",
        "\n",
        "    # Comparar con la pose anterior para detectar movimiento\n",
        "    if not hasattr(detectar_movimiento, \"pose_anterior\"):\n",
        "        detectar_movimiento.pose_anterior = keypoints_array\n",
        "        return False\n",
        "\n",
        "    # Calcular la distancia media entre los puntos clave actuales y los anteriores\n",
        "    diferencia = np.linalg.norm(detectar_movimiento.pose_anterior - keypoints_array, axis=1)\n",
        "    movimiento = np.mean(diferencia) > umbral_movimiento\n",
        "\n",
        "    # Actualizar la pose anterior\n",
        "    detectar_movimiento.pose_anterior = keypoints_array\n",
        "\n",
        "    return movimiento\n",
        "\n",
        "def analizar_pausas_y_recuperacion(video_path, umbral_inactividad=10):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error al abrir el video: {video_path}\")\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = frame_count / fps\n",
        "    print(f\"Duración del video: {duration} segundos\")\n",
        "\n",
        "    # Variables para almacenar las pausas y tiempos de inactividad\n",
        "    pausas = []\n",
        "    #inactividad = []\n",
        "\n",
        "    # Variables temporales para detectar pausas\n",
        "    inicio_pausa = None\n",
        "    duracion_pausa = 0\n",
        "    en_pausa = False\n",
        "\n",
        "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "        for i in range(frame_count):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            movimiento_detectado = detectar_movimiento(frame, pose)\n",
        "\n",
        "            if not movimiento_detectado:\n",
        "                if not en_pausa:\n",
        "                    inicio_pausa = i / fps\n",
        "                    en_pausa = True\n",
        "                duracion_pausa += 1 / fps\n",
        "            else:\n",
        "                if en_pausa and duracion_pausa >= umbral_inactividad:\n",
        "                    pausas.append((inicio_pausa, inicio_pausa + duracion_pausa))\n",
        "                inicio_pausa = None\n",
        "                duracion_pausa = 0\n",
        "                en_pausa = False\n",
        "\n",
        "        # Agregar la última pausa si terminó en el último frame\n",
        "        if en_pausa and duracion_pausa >= umbral_inactividad:\n",
        "            pausas.append((inicio_pausa, inicio_pausa + duracion_pausa))\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Imprimir resultados\n",
        "    print(\"Pausas detectadas:\")\n",
        "    for inicio, fin in pausas:\n",
        "        print(f\"Inicio: {inicio:.2f} segundos, Fin: {fin:.2f} segundos, Duración: {fin - inicio:.2f} segundos\")\n",
        "\n",
        "    return pausas\n",
        "\n",
        "# Ejemplo de uso:\n",
        "video_path = videos[0]\n",
        "pausas = analizar_pausas_y_recuperacion(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 2: Determinación de Tiempos de Recuperación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Factor de Recuperación (FR): 0.0273972602739726\n",
            "Tiempo Neto de Trabajo Repetitivo (TNTR): 365.0 minutos\n"
          ]
        }
      ],
      "source": [
        "def calcular_factor_recuperacion(pauses, duracion_turno, tiempo_almuerzo, tiempo_no_repetitivo):\n",
        "    # Calcular la duración total de las pausas en minutos\n",
        "    total_duracion_pausas = sum((pausa[1] - pausa[0]) for pausa in pauses) / 60.0  # Convertir de segundos a minutos\n",
        "\n",
        "    # Calcular el tiempo neto de trabajo repetitivo (TNTR)\n",
        "    TNTR = duracion_turno - (total_duracion_pausas + tiempo_almuerzo + tiempo_no_repetitivo)\n",
        "\n",
        "    # Calcular el Factor de Recuperación (FR)\n",
        "    # Esto dependerá de la fórmula o tabla específica que utilices\n",
        "    FR = total_duracion_pausas / TNTR  # Ejemplo simple, ajusta git según tu metodología\n",
        "\n",
        "    return FR, TNTR\n",
        "\n",
        "# Ejemplo de uso\n",
        "pauses = [(600, 900), (1800, 2100)]  # Pausas en segundos\n",
        "duracion_turno = 480  # Duración del turno en minutos\n",
        "tiempo_almuerzo = 45  # Tiempo de almuerzo en minutos\n",
        "tiempo_no_repetitivo = 60  # Tiempo de trabajo no repetitivo en minutos\n",
        "\n",
        "FR, TNTR = calcular_factor_recuperacion(pauses, duracion_turno, tiempo_almuerzo, tiempo_no_repetitivo)\n",
        "print(f\"Factor de Recuperación (FR): {FR}\")\n",
        "print(f\"Tiempo Neto de Trabajo Repetitivo (TNTR): {TNTR} minutos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Análisis de actividad, pausas y cálculo del Factor de Recuperación (FR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para identificar acciones técnicas dinámicas y estáticas\n",
        "def classify_technical_actions(keypoints):\n",
        "    atd = 0\n",
        "    ate = 0\n",
        "    dynamic_actions = 0\n",
        "    static_actions = 0\n",
        "\n",
        "    for i in range(1, len(keypoints)):\n",
        "        movement = keypoints[i] - keypoints[i - 1]\n",
        "        if movement.any():\n",
        "            dynamic_actions += 1\n",
        "        else:\n",
        "            static_actions += 1\n",
        "\n",
        "    # Calcular ATD según la tabla proporcionada\n",
        "    actions_per_minute = (dynamic_actions / (len(keypoints) / 30)) * 60\n",
        "    if actions_per_minute <= 20:\n",
        "        atd = 0\n",
        "    elif actions_per_minute <= 30:\n",
        "        atd = 1\n",
        "    elif actions_per_minute <= 40:\n",
        "        atd = 3\n",
        "    elif actions_per_minute <= 50:\n",
        "        atd = 4\n",
        "    elif actions_per_minute <= 60:\n",
        "        atd = 6\n",
        "    elif actions_per_minute <= 70:\n",
        "        atd = 8\n",
        "    else:\n",
        "        atd = 10\n",
        "\n",
        "    # Calcular ATE según la tabla proporcionada\n",
        "    if static_actions >= (2 / 3) * len(keypoints):\n",
        "        ate = 2.5\n",
        "    elif static_actions >= (3 / 3) * len(keypoints):\n",
        "        ate = 4.5\n",
        "\n",
        "    return atd, ate\n",
        "\n",
        "# Función para analizar la actividad y pausas en el video\n",
        "def analyze_activity_and_breaks(analysis_results, break_threshold=5):\n",
        "    activity_periods = []\n",
        "    break_periods = []\n",
        "\n",
        "    current_activity_start = None\n",
        "    current_break_start = None\n",
        "    in_activity = False\n",
        "\n",
        "    for second, keypoints in enumerate(analysis_results):\n",
        "        if keypoints:  # Si hay puntos clave detectados, se considera actividad\n",
        "            if not in_activity:\n",
        "                current_activity_start = second\n",
        "                if current_break_start is not None:\n",
        "                    break_duration = second - current_break_start\n",
        "                    if break_duration >= break_threshold:\n",
        "                        break_periods.append((current_break_start, second))\n",
        "                in_activity = True\n",
        "        else:  # Si no hay puntos clave detectados, se considera pausa\n",
        "            if in_activity:\n",
        "                activity_duration = second - current_activity_start\n",
        "                activity_periods.append((current_activity_start, second))\n",
        "                current_break_start = second\n",
        "                in_activity = False\n",
        "\n",
        "    # Agregar el último periodo de actividad o pausa\n",
        "    if in_activity and current_activity_start is not None:\n",
        "        activity_periods.append((current_activity_start, analysis_results[-1]))\n",
        "    if not in_activity and current_break_start is not None:\n",
        "        break_periods.append((current_break_start, analysis_results[-1]))\n",
        "\n",
        "    return activity_periods, break_periods\n",
        "\n",
        "# Análisis de actividad, pausas y cálculo del Factor de Frecuencia (FF)\n",
        "for video_path in videos:\n",
        "    analysis_results = analyze_keypoints(video_path)\n",
        "    activity_periods, break_periods = analyze_activity_and_breaks(analysis_results)\n",
        "\n",
        "    # Calcular el total de actividad y pausas\n",
        "    total_activity = sum(end - start for start, end in activity_periods)\n",
        "    total_break = sum(end - start for start, end in break_periods)\n",
        "    total_duration = total_activity + total_break\n",
        "\n",
        "    # Clasificar las acciones técnicas y calcular ATD y ATE\n",
        "    atd, ate = classify_technical_actions(analysis_results)\n",
        "\n",
        "    # Calcular el Factor de Frecuencia (FF)\n",
        "    ff = max(atd, ate)\n",
        "\n",
        "    # Imprimir los resultados\n",
        "    print(f\"Video: {os.path.basename(video_path)}\")\n",
        "    print(\"Periodos de Actividad:\", activity_periods)\n",
        "    print(\"Periodos de Pausas:\", break_periods)\n",
        "    print(f\"Total Actividad: {total_activity} segundos\")\n",
        "    print(f\"Total Pausas: {total_break} segundos\")\n",
        "    print(f\"Acciones Técnicas Dinámicas (ATD): {atd}\")\n",
        "    print(f\"Acciones Técnicas Estáticas (ATE): {ate}\")\n",
        "    print(f\"Factor de Frecuencia (FF): {ff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Factor fuerza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'analyze_video' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruta_al_video.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Ejecutar el análisis y calcular los factores\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[43manalyze_activity_and_calculate_factors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[18], line 70\u001b[0m, in \u001b[0;36manalyze_activity_and_calculate_factors\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_activity_and_calculate_factors\u001b[39m(video_path):\n\u001b[1;32m---> 70\u001b[0m     keypoints_list \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_video\u001b[49m(video_path)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Clasificar las acciones técnicas y calcular ATD y ATE\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     atd, ate \u001b[38;5;241m=\u001b[39m classify_technical_actions(keypoints_list)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'analyze_video' is not defined"
          ]
        }
      ],
      "source": [
        "import analyze_keypoints\n",
        "\n",
        "# Función para clasificar la fuerza aplicada en las acciones técnicas\n",
        "def classify_force_actions(technical_actions):\n",
        "    force_scores = []\n",
        "    for action in technical_actions:\n",
        "        # Aquí debes agregar tu lógica para evaluar la fuerza aplicada\n",
        "        # Por ahora, asumiremos fuerza moderada para todos los movimientos\n",
        "        force_scores.append(1)  # Cambia esto según tu evaluación de fuerza\n",
        "\n",
        "    # Calcular la puntuación de la fuerza (FFu) basada en la evaluación\n",
        "    if force_scores.count(4) > 0:\n",
        "        ffu = 4\n",
        "    elif force_scores.count(3) > 0:\n",
        "        ffu = 3\n",
        "    elif force_scores.count(2) > 0:\n",
        "        ffu = 2\n",
        "    elif force_scores.count(1) > 0:\n",
        "        ffu = 1\n",
        "    else:\n",
        "        ffu = 0\n",
        "\n",
        "    return ffu\n",
        "\n",
        "# Función para clasificar las acciones técnicas dinámicas y estáticas\n",
        "def classify_technical_actions(keypoints_list):\n",
        "    dynamic_actions = 0\n",
        "    static_actions = 0\n",
        "\n",
        "    for i in range(1, len(keypoints_list)):\n",
        "        if keypoints_list[i] is not None and keypoints_list[i - 1] is not None:\n",
        "            movement = np.linalg.norm(keypoints_list[i] - keypoints_list[i - 1])\n",
        "            if movement > 0:\n",
        "                dynamic_actions += 1\n",
        "            else:\n",
        "                static_actions += 1\n",
        "\n",
        "    # Calcular ATD según la tabla proporcionada\n",
        "    actions_per_minute = (dynamic_actions / (len(keypoints_list) / 30)) * 60\n",
        "    if actions_per_minute <= 20:\n",
        "        atd = 0\n",
        "    elif actions_per_minute <= 30:\n",
        "        atd = 1\n",
        "    elif actions_per_minute <= 40:\n",
        "        atd = 3\n",
        "    elif actions_per_minute <= 50:\n",
        "        atd = 4\n",
        "    elif actions_per_minute <= 60:\n",
        "        atd = 6\n",
        "    elif actions_per_minute <= 70:\n",
        "        atd = 8\n",
        "    else:\n",
        "        atd = 10\n",
        "\n",
        "    # Calcular ATE según la tabla proporcionada\n",
        "    if static_actions >= (2 / 3) * len(keypoints_list):\n",
        "        ate = 2.5\n",
        "    elif static_actions >= (3 / 3) * len(keypoints_list):\n",
        "        ate = 4.5\n",
        "    else:\n",
        "        ate = 0\n",
        "\n",
        "    return atd, ate\n",
        "\n",
        "# Análisis de actividad, pausas y cálculo del Factor de Frecuencia (FF)\n",
        "def analyze_activity_and_calculate_factors(video_path):\n",
        "    keypoints_list = analyze_video(video_path)\n",
        "\n",
        "    # Clasificar las acciones técnicas y calcular ATD y ATE\n",
        "    atd, ate = classify_technical_actions(keypoints_list)\n",
        "\n",
        "    # Calcular el Factor de Frecuencia (FF)\n",
        "    ff = max(atd, ate)\n",
        "\n",
        "    # Clasificar y calcular el Factor de Fuerza (FFu)\n",
        "    ffu = classify_force_actions(keypoints_list)\n",
        "\n",
        "    # Imprimir los resultados\n",
        "    print(f\"Video: {os.path.basename(video_path)}\")\n",
        "    print(f\"Acciones Técnicas Dinámicas (ATD): {atd}\")\n",
        "    print(f\"Acciones Técnicas Estáticas (ATE): {ate}\")\n",
        "    print(f\"Factor de Frecuencia (FF): {ff}\")\n",
        "    print(f\"Factor de Fuerza (FFu): {ffu}\")\n",
        "\n",
        "# Ruta del video\n",
        "video_path = \"ruta_al_video.mp4\"\n",
        "\n",
        "# Ejecutar el análisis y calcular los factores\n",
        "analyze_activity_and_calculate_factors(video_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMs43ccI7dJ7VPzWEYaM350",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
